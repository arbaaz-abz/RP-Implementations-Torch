{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f05576-5438-4535-9a85-5946bc331e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmi/miniforge3/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: German=8000, English=6000\n",
      "GERMAN | Index of <pad>: 0, <sos>: 1, <eos>: 2, <unk>: 3\n",
      "ENGLISH | Index of <pad>: 0, <sos>: 1, <eos>: 2, <unk>: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "random.seed(420)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataset import german_vocab, english_vocab, train_iterator, valid_iterator, test_iterator, en_pad_idx\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fd3ef5-8496-474f-a9fe-e5a06f0c65ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size, hidden_size, num_stacked_layers, embed_dropout, rnn_dropout):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_stacked_layers = num_stacked_layers\n",
    "\t\tself.embed_layer = nn.Embedding(vocab_size, embed_size)\n",
    "\t\tself.lstm = nn.LSTM(embed_size, hidden_size, num_stacked_layers, batch_first=True, dropout=rnn_dropout)\n",
    "\t\tself.dropout = nn.Dropout(embed_dropout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# x -> batch_size, sequences\n",
    "        \n",
    "\t\t# embed -> batch_size, sequences, embed_size\n",
    "\t\tembed = self.dropout(self.embed_layer(x))\n",
    "\n",
    "\t\tb_size = x.shape[0]\n",
    "\t\th0 = torch.zeros([self.num_stacked_layers, b_size, self.hidden_size])\n",
    "\t\tc0 = torch.zeros([self.num_stacked_layers, b_size, self.hidden_size])\n",
    "\t\t# out - Hidden state at each timestep: (b_size, timesteps, hidden_size)\n",
    "        # _ : Final hidden state and cell state (for each LSTM layer)\n",
    "\t\tout, (hidden, cell) = self.lstm(embed, (h0, c0))\n",
    "\t\treturn hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size, hidden_size, num_stacked_layers, embed_dropout, rnn_dropout):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_stacked_layers = num_stacked_layers\n",
    "\t\tself.embed_layer = nn.Embedding(vocab_size, embed_size)\n",
    "\t\tself.lstm = nn.LSTM(embed_size, hidden_size, num_stacked_layers, batch_first=True, dropout=rnn_dropout)\n",
    "\t\tself.dropout = nn.Dropout(embed_dropout)\n",
    "\t\tself.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\tdef forward(self, x, hidden, cell):\n",
    "\t\t# x -> (batch_size, 1)\n",
    "        \n",
    "\t\t# embed -> (batch_size, 1, embed_size)\n",
    "\t\tembed = self.dropout(self.embed_layer(x))\n",
    "\n",
    "\t\t# hidden -> (num_stacked_layers, batch_size, hidden_state)\n",
    "\t\tout, (hidden, cell) = self.lstm(embed, (hidden, cell))\n",
    "\n",
    "\t\t# out -> (batch_size, sequences, hidden_state) \n",
    "\t\t# x -> (batch_size, output_vocab_size)\n",
    "\t\tx = self.fc(out).squeeze()\n",
    "\t\treturn x, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder):\n",
    "\t\tsuper(Seq2Seq, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\t\n",
    "\tdef forward(self, source, target, teacher_force_ratio=0.5):\n",
    "\t\t# source -> (batch_size, sequences)\n",
    "\t\tb_size = source.shape[0]\n",
    "\t\ttarget_len = target.shape[1]\n",
    "\t\ttarget_vocab_size = len(english_vocab)\n",
    "\n",
    "\t\toutputs = torch.zeros(target_len, b_size, target_vocab_size).to(device)\n",
    "\n",
    "\t\t# Encode the source sentence\n",
    "\t\thidden, cell = self.encoder(source)\n",
    "\n",
    "\t\t# First token\n",
    "\t\tx = target[:, 0]\n",
    "\n",
    "\t\tfor t in range(1, target_len):\n",
    "\t\t\toutput, hidden, cell = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "\t\t\toutputs[t] = output\n",
    "\n",
    "\t\t\t# predicted_tokens (indices) -> (batch_size,)\n",
    "\t\t\tpredicted_tokens = output.argmax(1)\n",
    "\t\t\tx = target[:, t] if random.random() < teacher_force_ratio else predicted_tokens\n",
    "\t\treturn outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad16ebc-93a3-4dd9-8d58-161cbce4391a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "lr = 0.001\n",
    "load_model = False\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"mps\")\n",
    "input_encoder_size = len(german_vocab)\n",
    "input_decoder_size = len(english_vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "lstm_stacked_layers = 2\n",
    "embed_dropout = 0.5\n",
    "lstm_dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ac6ce6-2420-45c1-bf8e-7237aa40831a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_net = Encoder(\n",
    "    input_encoder_size, \n",
    "    encoder_embedding_size, \n",
    "    hidden_size, \n",
    "    lstm_stacked_layers, \n",
    "    embed_dropout, \n",
    "    lstm_dropout\n",
    ").to(device)\n",
    "\n",
    "# Decoder\n",
    "decoder_net = Decoder(\n",
    "    input_decoder_size,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    lstm_stacked_layers,\n",
    "    embed_dropout,\n",
    "    lstm_dropout\n",
    ").to(device)\n",
    "\n",
    "# Wrapper model\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=en_pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint_9.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906e269b-c05d-4d8c-897e-63406a7b068b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters (ENCODER): 16228096\n",
      "Total parameters (DECODER): 21778096\n",
      "Total parameters (COMPLETE): 38006192\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in encoder_net.parameters())\n",
    "print(\"Total parameters (ENCODER):\", total_params)\n",
    "\n",
    "total_params = sum(p.numel() for p in decoder_net.parameters())\n",
    "print(\"Total parameters (DECODER):\", total_params)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters (COMPLETE):\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7914cca-a3ae-407c-a9ca-7d3bc38ec8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "writer = SummaryWriter(\"runs/plot_loss\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f047f-b670-4609-957f-bbdecf8d333f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1813/1813 [8:18:44<00:00, 16.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.5367731829736355\n",
      "=> Saving checkpoint\n",
      "Checkpoint saved\n",
      "Epoch: 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1813/1813 [2:29:55<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.493858135410652\n",
      "=> Saving checkpoint\n",
      "Checkpoint saved\n",
      "Epoch: 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 1500/1813 [28:52<06:33,  1.26s/it]"
     ]
    }
   ],
   "source": [
    "model.train(True)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}/{epochs}\")\n",
    "    running_loss = 0.\n",
    "    for src, trg, _, _ in tqdm(train_iterator):\n",
    "        inp_data = src.permute(1, 0) # (batch_size, src_len)\n",
    "        target = trg.permute(1, 0) # (batch_size, trg_len)\n",
    "        \n",
    "        output = model(inp_data.to(device), target.to(device))  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = output.reshape(-1, output.shape[2]) # (trg_len * batch_size, output_vocab_size)\n",
    "        target = target.reshape(-1) # (trg_len * batch_size)\n",
    "        loss = loss_function(output, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipping gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "         \n",
    "    print(\"Loss: \", running_loss/len(train_iterator))        \n",
    "    # Save model\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optim\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, f\"my_checkpoint_{epoch}.pth.tar\")\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011bf29-5a91-4cd4-bf8b-69bf7201ac51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
